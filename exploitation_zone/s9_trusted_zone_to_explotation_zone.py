#  Exploitation zone creation
## Moving data tables from the trusted zone to the exploitation zone
### In the explotation zone data quality processes and data integration takes place
### This script automatically creates the exploitation zone database
from functools import reduce
import duckdb
import pandas as pd
from utilities.os_utilities import getDataSourcesNames, createDirectory
from utilities.db_utilities import getDataframeFrom_trusted_noNAs, saveDataframeTo_exploitation_year_and_country, saveDataframeTo_exploitation
from paths import temporalPath, exploitationDatabasesDir


def dataQualityProcesses ():
    """
    # Creation of explotation zone database
    ## Data quality processes for integration - NCEI
    ### In this part NCEI data get aggregated to yearly level so they can be joined with WEB data 
    """

    # Getting the names of the different data sources
    data_sources_names = getDataSourcesNames(temporalPath)

    createDirectory(exploitationDatabasesDir)


    # Data quality processes for integration for each data source
    for data_source_name in data_sources_names:
        print(f"Data quality processes for integration - {data_source_name} data source \n")
        
        df = getDataframeFrom_trusted_noNAs(data_source_name)

        if data_source_name == "NCEI":
            abb_dic = {"BE" : "Belgium", "JA": "Japan"}
            print(f"Aggregating - {data_source_name} data to YEARLY - COUNTRY LEVEL \n")
            df['YEAR'] = pd.DatetimeIndex(df['DATE']).year.astype(int)
            df['COUNTRY'] = df['NAME'].str[-2:].replace(abb_dic)
            df = df.drop(["LATITUDE", "LONGITUDE", "ELEVATION", "MAX_ATTRIBUTES","MIN_ATTRIBUTES", "PRCP_ATTRIBUTES", "FRSHTT", "STATION"], axis = 1)
            agg_df = df.groupby(['YEAR', 'COUNTRY'], as_index=False).agg("mean")
            print(agg_df.head())
            saveDataframeTo_exploitation_year_and_country(agg_df, data_source_name)

        elif data_source_name == "WEB":
            
            print(f"Selecting specific years and countries from the trusted database of {data_source_name} data source \n")
            years = ["1971", "1972", "1973", "1974", "1975", "1976", "1977", "1978", "1979", "1980", "1981", "1982", "1983", "1984", "1985", "1986", "1987", "1988", "1989", "1990", "1991", "1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019"]
            countries = ["Belgium", "Japan"]
            interesting_cols = ["Country","Product", "Flow"]
            final_cols = interesting_cols + years
            df = df[final_cols]
            df = df.loc[df["Country"].isin(countries)]
            print(df.head())
            saveDataframeTo_exploitation_year_and_country(df, data_source_name)

def dataIntegration ():
    # Getting the names of the different data sources
    data_sources_names = getDataSourcesNames(temporalPath)

    df_list =[]
    # Data integration from each data source to a single view table in exploitation database
    for data_source_name in data_sources_names:
        try:
            print(f"Data integration process \n")
            con = duckdb.connect(database=f'{exploitationDatabasesDir}{data_source_name}_exploitation_year_and_country.duckdb', read_only=False)
            df = con.execute(f'SELECT * FROM {data_source_name}').fetchdf()
            if data_source_name == "NCEI":
                print(df.head())
            elif data_source_name == "WEB":
                print(df.head())
                df["Var"] = df["Product"] + " - " + df["Flow"]
                unique_years_length = len(df.columns[3:-1].to_list())
                unique_countries = df["Country"].unique()
                list_of_dicts = []
                for c in range(unique_years_length):
                    year = df.columns[3 + c]
                    for country in unique_countries:
                        temp_dict = {}
                        for c1,var in enumerate(df["Var"].unique()):
                            product = var.split(" - ")[0]
                            flow = var.split(" - ")[1]
                            for _,row in df.iterrows():
                                if (row["Country"] == country and row["Product"] ==product and row["Flow"] == flow):
                                    temp_dict["YEAR"] = year
                                    temp_dict["COUNTRY"] = country
                                    temp_dict[var] = row[year]
                                    break
                        list_of_dicts.append(temp_dict)
                web_df_list =[]
                for dic in list_of_dicts:
                    df_temp = pd.DataFrame(dic, index=[0])
                    web_df_list.append(df_temp)
                df = reduce(lambda df1,df2: pd.concat([df1, df2]), web_df_list)
            con.close()
            df_list.append(df)
        except Exception as e:
            print(e)
            con.close()
    if len(df_list) > 1:
        for df in df_list:
            df["YEAR"] = pd.to_numeric(df["YEAR"])
        df = reduce(lambda df1,df2: pd.merge(df1,df2, on=["YEAR", "COUNTRY"]), df_list)
    else:
        df = df_list[0]
    saveDataframeTo_exploitation(df)
    

def main():
    dataQualityProcesses()
    dataIntegration()

if __name__ == "__main__":
    main()